{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xn2EYLbArKIE"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1AFx8YVrLsB"
   },
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchsummary\n",
    "%pip install matplotlib\n",
    "%pip install pillow\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ojrPcD9u3ekW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "import pandas as pd\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hz7D0WafuXYu"
   },
   "source": [
    "## Let's download Cifar10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B4Y2yq5rtTp9",
    "outputId": "41d787f0-c44c-408f-bbdb-2015e9ae0f5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|â–Ž                                                                 | 753664/170498071 [00:17<1:06:38, 42454.59it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# We're only using this to download dataset. we will manually create a dataset from downloaded files\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mCIFAR10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./datasets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\cifar.py:65\u001b[0m, in \u001b[0;36mCIFAR10.__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\cifar.py:139\u001b[0m, in \u001b[0;36mCIFAR10.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles already downloaded and verified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgz_md5\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\utils.py:434\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[0;32m    432\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[1;32m--> 434\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\utils.py:144\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fpath)\n\u001b[1;32m--> 144\u001b[0m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\utils.py:48\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[1;34m(url, filename, chunk_size)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_urlretrieve\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m, chunk_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m---> 48\u001b[0m         \u001b[43m_save_response_content\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\utils.py:37\u001b[0m, in \u001b[0;36m_save_response_content\u001b[1;34m(content, destination, length)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_save_response_content\u001b[39m(\n\u001b[0;32m     32\u001b[0m     content: Iterator[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m     33\u001b[0m     destination: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     34\u001b[0m     length: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(destination, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total\u001b[38;5;241m=\u001b[39mlength) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m---> 37\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m content:\n\u001b[0;32m     38\u001b[0m             \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk:\n\u001b[0;32m     40\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\utils.py:48\u001b[0m, in \u001b[0;36m_urlretrieve.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_urlretrieve\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m, chunk_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m---> 48\u001b[0m         _save_response_content(\u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m), filename, length\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mlength)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We're only using this to download dataset. we will manually create a dataset from downloaded files\n",
    "data = CIFAR10(root='./datasets', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZwniEXr_A3D"
   },
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "\n",
    "  def __init__(self,data,transform,train=True):\n",
    "    self.dataset = data\n",
    "    self.transform=transform\n",
    "    self.train = train\n",
    "\n",
    "    self.imgdf_list = self.extract_image_and_create_list()\n",
    "\n",
    "\n",
    "  def extract_image_and_create_list(self):\n",
    "\n",
    "    dataset_path = \"cifar_images\"\n",
    "    if not os.path.isdir(dataset_path):\n",
    "      os.mkdir(dataset_path)\n",
    "    dataset_len = len(self.dataset)\n",
    "\n",
    "    image_path_list = []\n",
    "    image_label_list = []\n",
    "    if self.train:\n",
    "      start_idx = 0\n",
    "      end_idx = 30000\n",
    "    else:\n",
    "      start_idx = 30000\n",
    "      end_idx = dataset_len\n",
    "\n",
    "    for i in range(start_idx,end_idx):\n",
    "      img,label = self.dataset[i]\n",
    "\n",
    "      save_image_path = dataset_path+\"/image_\"+str(i)+\".png\"\n",
    "\n",
    "      img.save(save_image_path)\n",
    "\n",
    "      image_path_list.append(save_image_path)\n",
    "      image_label_list.append(label)\n",
    "\n",
    "    imgdf = pd.DataFrame({\"image_path\":image_path_list, \"image_label\":image_label_list})\n",
    "    print(imgdf)\n",
    "    return imgdf\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    df_row = self.imgdf_list.iloc[index]\n",
    "\n",
    "    image_path = df_row[\"image_path\"]\n",
    "    label = df_row[\"image_label\"]\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    image = self.transform(image)\n",
    "\n",
    "\n",
    "    return image,label\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.imgdf_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5buFpBiy_BkV",
    "outputId": "eec58d61-6d91-48fc-9e69-e4d58642f393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         image_path  image_label\n",
      "0          cifar_images/image_0.png            6\n",
      "1          cifar_images/image_1.png            9\n",
      "2          cifar_images/image_2.png            9\n",
      "3          cifar_images/image_3.png            4\n",
      "4          cifar_images/image_4.png            1\n",
      "...                             ...          ...\n",
      "29995  cifar_images/image_29995.png            2\n",
      "29996  cifar_images/image_29996.png            5\n",
      "29997  cifar_images/image_29997.png            3\n",
      "29998  cifar_images/image_29998.png            7\n",
      "29999  cifar_images/image_29999.png            1\n",
      "\n",
      "[30000 rows x 2 columns]\n",
      "                         image_path  image_label\n",
      "0      cifar_images/image_30000.png            0\n",
      "1      cifar_images/image_30001.png            6\n",
      "2      cifar_images/image_30002.png            0\n",
      "3      cifar_images/image_30003.png            2\n",
      "4      cifar_images/image_30004.png            7\n",
      "...                             ...          ...\n",
      "19995  cifar_images/image_49995.png            2\n",
      "19996  cifar_images/image_49996.png            6\n",
      "19997  cifar_images/image_49997.png            9\n",
      "19998  cifar_images/image_49998.png            1\n",
      "19999  cifar_images/image_49999.png            1\n",
      "\n",
      "[20000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "mytranform = transforms.Compose([ transforms.ToTensor(),\n",
    "                                 transforms.Normalize(std=(0.5, 0.5, 0.5),\n",
    "                                                      mean=(0.5, 0.5, 0.5)) ])\n",
    "\n",
    "train_dataset = myDataset(data=data, transform=mytranform,train=True)\n",
    "val_dataset = myDataset(data=data, transform=mytranform,train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwHWyUmv_BUt"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  # no need to shuffle validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfqMUx3u_BNK"
   },
   "outputs": [],
   "source": [
    "class Cifar10Classifier(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    super(Cifar10Classifier, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1,stride=1)\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "    self.relu = nn.ReLU()  # Relu isn't learnable. no need to intialize different relu objects for each layer\n",
    "    self.pool = nn.MaxPool2d(2, 2)  # pool isn't learnable to no need to initialize different pool layers unless we want to change window size\n",
    "\n",
    "    self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "    self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    single_input = False\n",
    "    if x.ndim == 3:  # 3 dimensions mean [C, H, W] instead of [B, C, H, W] so we're dealing with a single image\n",
    "      x = x.unsqueeze(dim=0)  # convert [C, H, W] to [1, C, H, W] where 1 will act as batch size\n",
    "\n",
    "      # keep track of whether input was one (non-batch) image.\n",
    "      # If so, we want to convert it back to the same format after inference for consistency purposes\n",
    "      single_input = True\n",
    "    x = self.pool(self.relu(self.conv1(x)))\n",
    "    x = self.pool(self.relu(self.conv2(x)))\n",
    "    x = self.pool(self.relu(self.conv3(x)))\n",
    "    x = x.flatten(start_dim=1)\n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    x = self.softmax(x)\n",
    "\n",
    "\n",
    "    if single_input:\n",
    "      x = x.squeeze(dim=0)  # or x = x[0]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORd9kkpY_BFV"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # checks if machine supports cuda and if it does, we use that, otherwise cpu\n",
    "model = Cifar10Classifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uw0puegszn-n",
    "outputId": "0a02ddc4-034a-4c2a-caa5-ed47d582b782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "num_epochs = 30\n",
    "lr = 1e-4\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()  # multi-class\n",
    "\n",
    "model.to(device)  # we need to send all input tensors as well as our model to this device. by default they are on cpu\n",
    "\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QO7JCF_Fzw5-",
    "outputId": "a1b33bd8-a17a-4a6b-fc3c-c8e9256af60d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train_loss=2.163019166692098, val_loss=2.1003595169067384. labelled 7183/20000 correctly (35.915% accuracy)\n",
      "Epoch: 1, train_loss=2.0722880416870115, val_loss=2.0555192184448243. labelled 8128/20000 correctly (40.64% accuracy)\n",
      "Epoch: 2, train_loss=2.0347116586049396, val_loss=2.0234809391021726. labelled 8728/20000 correctly (43.64% accuracy)\n",
      "Epoch: 3, train_loss=2.0094847787221273, val_loss=2.022584180831909. labelled 8688/20000 correctly (43.44% accuracy)\n",
      "Epoch: 4, train_loss=1.992315344619751, val_loss=2.006229560470581. labelled 9091/20000 correctly (45.455% accuracy)\n",
      "Epoch: 5, train_loss=1.9745855347951253, val_loss=1.9889001819610597. labelled 9432/20000 correctly (47.160000000000004% accuracy)\n",
      "Epoch: 6, train_loss=1.9611418661117554, val_loss=1.9802022159576416. labelled 9555/20000 correctly (47.775% accuracy)\n",
      "Epoch: 7, train_loss=1.9462307881037395, val_loss=1.9757911838531494. labelled 9665/20000 correctly (48.325% accuracy)\n",
      "Epoch: 8, train_loss=1.934156399790446, val_loss=1.9530572002410889. labelled 10163/20000 correctly (50.815% accuracy)\n",
      "Epoch: 9, train_loss=1.921652293841044, val_loss=1.9495847957611083. labelled 10224/20000 correctly (51.12% accuracy)\n",
      "Epoch: 10, train_loss=1.9110440996805826, val_loss=1.9366630741119384. labelled 10522/20000 correctly (52.61% accuracy)\n",
      "Epoch: 11, train_loss=1.9011807126363118, val_loss=1.92169469871521. labelled 10789/20000 correctly (53.945% accuracy)\n",
      "Epoch: 12, train_loss=1.8892267456054688, val_loss=1.9168230895996095. labelled 10888/20000 correctly (54.44% accuracy)\n",
      "Epoch: 13, train_loss=1.8788226130167642, val_loss=1.9062377170562743. labelled 11093/20000 correctly (55.464999999999996% accuracy)\n",
      "Epoch: 14, train_loss=1.8710086023330688, val_loss=1.8984241437911988. labelled 11286/20000 correctly (56.43% accuracy)\n",
      "Epoch: 15, train_loss=1.8607058118184407, val_loss=1.8942945510864257. labelled 11335/20000 correctly (56.675% accuracy)\n",
      "Epoch: 16, train_loss=1.8521761018753051, val_loss=1.8889257793426513. labelled 11427/20000 correctly (57.135000000000005% accuracy)\n",
      "Epoch: 17, train_loss=1.8447621351242065, val_loss=1.8857752145767213. labelled 11514/20000 correctly (57.57% accuracy)\n",
      "Epoch: 18, train_loss=1.8335395095189413, val_loss=1.8732287517547608. labelled 11763/20000 correctly (58.815% accuracy)\n",
      "Epoch: 19, train_loss=1.8257512011845907, val_loss=1.8715067092895508. labelled 11837/20000 correctly (59.185% accuracy)\n",
      "Epoch: 20, train_loss=1.8169248587926228, val_loss=1.8644207344055175. labelled 11946/20000 correctly (59.730000000000004% accuracy)\n",
      "Epoch: 21, train_loss=1.8080901717503866, val_loss=1.8637108392715453. labelled 11965/20000 correctly (59.824999999999996% accuracy)\n",
      "Epoch: 22, train_loss=1.8024547468185426, val_loss=1.8572838956832887. labelled 12098/20000 correctly (60.49% accuracy)\n",
      "Epoch: 23, train_loss=1.7941672070185344, val_loss=1.8584080156326295. labelled 12071/20000 correctly (60.355000000000004% accuracy)\n",
      "Epoch: 24, train_loss=1.787055083656311, val_loss=1.849383790588379. labelled 12265/20000 correctly (61.324999999999996% accuracy)\n",
      "Epoch: 25, train_loss=1.7792519432703653, val_loss=1.8463512832641602. labelled 12339/20000 correctly (61.695% accuracy)\n",
      "Epoch: 26, train_loss=1.7730624247868856, val_loss=1.838675192642212. labelled 12497/20000 correctly (62.485% accuracy)\n",
      "Epoch: 27, train_loss=1.765671379407247, val_loss=1.8373662837982179. labelled 12502/20000 correctly (62.51% accuracy)\n",
      "Epoch: 28, train_loss=1.758011117362976, val_loss=1.8319960243225097. labelled 12573/20000 correctly (62.865% accuracy)\n",
      "Epoch: 29, train_loss=1.7511036430358886, val_loss=1.8341403356552124. labelled 12577/20000 correctly (62.885000000000005% accuracy)\n",
      "Training complete on device cpu.\n",
      "CPU times: user 40min 32s, sys: 31.6 s, total: 41min 4s\n",
      "Wall time: 41min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch_no in range(num_epochs):\n",
    "\n",
    "  model.train()  # convert to train model. This turns out train-specific layers in the model (if you dont know about them, an example of them is dropout. more on this later)\n",
    "\n",
    "  epoch_weighted_loss = 0\n",
    "\n",
    "  for batch_X, batch_y in train_loader:\n",
    "\n",
    "    batch_X = batch_X.to(device)\n",
    "    batch_y = batch_y.to(device)\n",
    "\n",
    "    batch_y_probs = model(batch_X)  # outputs [N, 10] where each [:, 10] is probabilities for class (0-9)\n",
    "\n",
    "    loss = criterion(batch_y_probs, batch_y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_weighted_loss += (len(batch_y)*loss.item())\n",
    "\n",
    "  epoch_loss = epoch_weighted_loss/len(train_loader.dataset)\n",
    "  train_losses.append(epoch_loss)\n",
    "\n",
    "\n",
    "  # validation time\n",
    "\n",
    "  model.eval()  # take model to evaluation mode. turn off train-only layers\n",
    "  correctly_labelled = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "\n",
    "    val_epoch_weighted_loss = 0\n",
    "\n",
    "    for val_batch_X, val_batch_y in val_loader:\n",
    "\n",
    "      val_batch_X = val_batch_X.to(device)\n",
    "      val_batch_y = val_batch_y.to(device)\n",
    "\n",
    "      val_batch_y_probs = model(val_batch_X)\n",
    "\n",
    "      loss = criterion(val_batch_y_probs, val_batch_y)\n",
    "      val_epoch_weighted_loss += (len(val_batch_y)*loss.item())\n",
    "\n",
    "      val_batch_y_pred = val_batch_y_probs.argmax(dim=1)  # convert probailities to labels by picking the label (index) with the highest prob\n",
    "\n",
    "      correctly_labelled += (val_batch_y_pred == val_batch_y).sum().item()  # item converts tensor to float/int/list\n",
    "\n",
    "  val_epoch_loss = val_epoch_weighted_loss/len(val_loader.dataset)\n",
    "  val_losses.append(val_epoch_loss)\n",
    "\n",
    "  print(f'Epoch: {epoch_no}, train_loss={epoch_loss}, val_loss={val_epoch_loss}. labelled {correctly_labelled}/{len(val_loader.dataset)} correctly ({correctly_labelled/len(val_loader.dataset)*100}% accuracy)')\n",
    "\n",
    "print(f'Training complete on device {device}.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
